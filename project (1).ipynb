{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\n",
    "https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/classifier/classifier.py\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HC: decision tree, kNN, Random Forest\n",
    "\n",
    "Moruf: logistic regression, Naive Bayes, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ching/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ching/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ching/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ching/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read dataset1 csv file\n",
    "\n",
    "data_tweet = pd.read_csv(\"/Users/ching/Desktop/Class/Data Project Management/AI and Big Data Project/Class 1/Dataset1_labeled_data.csv\", sep = \",\")\n",
    "\n",
    "data_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          24783 non-null  int64 \n",
      " 1   count               24783 non-null  int64 \n",
      " 2   hate_speech         24783 non-null  int64 \n",
      " 3   offensive_language  24783 non-null  int64 \n",
      " 4   neither             24783 non-null  int64 \n",
      " 5   class               24783 non-null  int64 \n",
      " 6   tweet               24783 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# summary of the dataset\n",
    "\n",
    "data_tweet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        !!! rt @mayasolovely: as a woman you shouldn't...\n",
       "1        !!!!! rt @mleew17: boy dats cold...tyga dwn ba...\n",
       "2        !!!!!!! rt @urkindofbrand dawg!!!! rt @80sbaby...\n",
       "3        !!!!!!!!! rt @c_g_anderson: @viva_based she lo...\n",
       "4        !!!!!!!!!!!!! rt @shenikaroberts: the shit you...\n",
       "                               ...                        \n",
       "24778    you's a muthaf***in lie &#8220;@lifeasking: @2...\n",
       "24779    you've gone and broke the wrong heart baby, an...\n",
       "24780    young buck wanna eat!!.. dat nigguh like i ain...\n",
       "24781                youu got wild bitches tellin you lies\n",
       "24782    ~~ruffled | ntac eileen dahlia - beautiful col...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the tweet column, make all words to lowercase, and remove redundant space\n",
    "\n",
    "data_tweet_str = data_tweet['tweet'].apply(lambda x: ' '.join(x.lower().strip() for x in x.split()))\n",
    "data_tweet_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         rt mayasolovely as a woman you shouldnt compl...\n",
       "1         rt mleew17 boy dats coldtyga dwn bad for cuff...\n",
       "2         rt urkindofbrand dawg rt 80sbaby4life you eve...\n",
       "3         rt c_g_anderson viva_based she look like a tr...\n",
       "4         rt shenikaroberts the shit you hear about me ...\n",
       "                               ...                        \n",
       "24778    yous a muthafin lie 8220lifeasking 20_pearls c...\n",
       "24779    youve gone and broke the wrong heart baby and ...\n",
       "24780    young buck wanna eat dat nigguh like i aint fu...\n",
       "24781                youu got wild bitches tellin you lies\n",
       "24782    ruffled  ntac eileen dahlia  beautiful color c...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all punctuations\n",
    "\n",
    "data_tweet_str_wo_punc = data_tweet_str.str.replace('[^\\w\\s]','')\n",
    "data_tweet_str_wo_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the stopword list\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'english',\n",
       " 'aint',\n",
       " 'rt',\n",
       " 'shouldnt',\n",
       " 'ya',\n",
       " 'yea',\n",
       " 'another',\n",
       " 'yo',\n",
       " 'us',\n",
       " 'cant',\n",
       " 'im',\n",
       " 'yeah',\n",
       " 'doesnt',\n",
       " 'tweets',\n",
       " 'tweet',\n",
       " 'lmao',\n",
       " 'Lmao',\n",
       " 'LMAO',\n",
       " 'u',\n",
       " 'dont',\n",
       " 'lol',\n",
       " 'amp',\n",
       " 'thats',\n",
       " 'youre',\n",
       " 'wont',\n",
       " 'would']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-define a stopword list \n",
    "\n",
    "more_stopwords = [\"english\", \"aint\", \"rt\", 'shouldnt', 'ya', 'yea', 'another', 'yo', 'us', 'cant', 'im', 'yeah', \\\n",
    "                  'doesnt', 'tweets', 'tweet', 'lmao', 'Lmao', 'LMAO', 'u', 'dont', 'lol', 'amp', 'thats', 'youre', \\\n",
    "                  'wont', 'would']\n",
    "\n",
    "# combine the two stopword lists together\n",
    "\n",
    "for i in range(len(more_stopwords)):\n",
    "    stop.append(more_stopwords[i])\n",
    "\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        mayasolovely woman complain cleaning house man...\n",
       "1        mleew17 boy dats coldtyga dwn bad cuffin dat h...\n",
       "2        urkindofbrand dawg 80sbaby4life ever fuck bitc...\n",
       "3                 c_g_anderson viva_based look like tranny\n",
       "4        shenikaroberts shit hear might true might fake...\n",
       "                               ...                        \n",
       "24778    yous muthafin lie 8220lifeasking 20_pearls cor...\n",
       "24779    youve gone broke wrong heart baby drove rednec...\n",
       "24780      young buck wanna eat dat nigguh like fuckin dis\n",
       "24781                    youu got wild bitches tellin lies\n",
       "24782    ruffled ntac eileen dahlia beautiful color com...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "data_tweet_str_wo_punc_wo_stop = data_tweet_str_wo_punc.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data_tweet_str_wo_punc_wo_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        mayasolovely woman complain cleaning house man...\n",
       "1        mleew17 boy dats coldtyga dwn bad cuffin dat h...\n",
       "2        urkindofbrand dawg 80sbaby4life ever fuck bitc...\n",
       "3                 c_g_anderson viva_based look like tranny\n",
       "4        shenikaroberts shit hear might true might fake...\n",
       "                               ...                        \n",
       "24778    yous muthafin lie 8220lifeasking 20_pearls cor...\n",
       "24779    youve gone broke wrong heart baby drove rednec...\n",
       "24780      young buck wanna eat dat nigguh like fuckin dis\n",
       "24781                    youu got wild bitches tellin lies\n",
       "24782    ruffled ntac eileen dahlia beautiful color com...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove web address\n",
    "\n",
    "data_tweet_str_wo_punc_wo_stop_wo_http = data_tweet_str_wo_punc_wo_stop.apply(lambda x: \" \".join(x for x in x.split() \\\n",
    "                                                                                                 if x[0:4] != 'http'))\n",
    "data_tweet_str_wo_punc_wo_stop_wo_http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        mayasolovely woman complain cleaning house man...\n",
       "1        mleew17 boy dats coldtyga dwn bad cuffin dat h...\n",
       "2        urkindofbrand dawg ever fuck bitch start cry c...\n",
       "3                 c_g_anderson viva_based look like tranny\n",
       "4        shenikaroberts shit hear might true might fake...\n",
       "                               ...                        \n",
       "24778    yous muthafin lie corey_emanuel right tl trash...\n",
       "24779    youve gone broke wrong heart baby drove rednec...\n",
       "24780      young buck wanna eat dat nigguh like fuckin dis\n",
       "24781                    youu got wild bitches tellin lies\n",
       "24782    ruffled ntac eileen dahlia beautiful color com...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove numbers and words starting with numbers\n",
    "\n",
    "number = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
    "\n",
    "data_tweet_str_wo_punc_wo_stop_wo_http_wo_no = data_tweet_str_wo_punc_wo_stop_wo_http\\\n",
    "                                                .apply(lambda x: \" \".join(x for x in x.split() \\\n",
    "                                                if x[0] not in number))\n",
    "data_tweet_str_wo_punc_wo_stop_wo_http_wo_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got bitch tip toeing hardwood floors 128514 httptcocou2wq5l4q\n",
      "got bitch tip toeing hardwood floors 128514\n",
      "got bitch tip toeing hardwood floors \n",
      "\n",
      "yous muthafin lie 8220lifeasking 20_pearls corey_emanuel right tl trash 8230 mine bible scriptures hymns8221\n",
      "yous muthafin lie corey_emanuel right tl trash mine bible scriptures hymns8221\n"
     ]
    }
   ],
   "source": [
    "# check data cleaning results\n",
    "\n",
    "check1 = 24\n",
    "check2 = 24778\n",
    "\n",
    "print(data_tweet_str_wo_punc_wo_stop[check1])\n",
    "\n",
    "print(data_tweet_str_wo_punc_wo_stop_wo_http[check1])\n",
    "\n",
    "print(data_tweet_str_wo_punc_wo_stop_wo_http_wo_no[check1], '\\n')\n",
    "\n",
    "print(data_tweet_str_wo_punc_wo_stop[check2])\n",
    "\n",
    "print(data_tweet_str_wo_punc_wo_stop_wo_http_wo_no[check2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bitch      8120\n",
       "bitches    3045\n",
       "like       2759\n",
       "hoes       2334\n",
       "pussy      2073\n",
       "hoe        1866\n",
       "ass        1562\n",
       "get        1428\n",
       "fuck       1406\n",
       "got        1289\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most frequent words\n",
    "\n",
    "freq = pd.Series(' '.join(data_tweet_str_wo_punc_wo_stop_wo_http_wo_no).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gabrielle                                1\n",
       "ajumma                                   1\n",
       "bliss                                    1\n",
       "arrests                                  1\n",
       "midwestmiles                             1\n",
       "stanky                                   1\n",
       "bounty                                   1\n",
       "macktown                                 1\n",
       "prediction                               1\n",
       "ktpowers98                               1\n",
       "mchastain81                              1\n",
       "fam128564                                1\n",
       "kilmeade                                 1\n",
       "selectively                              1\n",
       "tensing                                  1\n",
       "lenny                                    1\n",
       "nonphysical                              1\n",
       "nastycopper                              1\n",
       "fraudulent                               1\n",
       "nathaniel                                1\n",
       "crept                                    1\n",
       "datboysmokeyy                            1\n",
       "in128544                                 1\n",
       "slowjams                                 1\n",
       "umma                                     1\n",
       "kraut                                    1\n",
       "istricer                                 1\n",
       "dollarigns                               1\n",
       "way12853082211285141285141285148221so    1\n",
       "wife8217s                                1\n",
       "vintagebaublesnbits                      1\n",
       "savoury                                  1\n",
       "ohi                                      1\n",
       "ksteven37                                1\n",
       "mykel                                    1\n",
       "hattiesburg                              1\n",
       "streetart                                1\n",
       "thebossunicorn_                          1\n",
       "lovespell                                1\n",
       "virginiayumyum                           1\n",
       "sorries                                  1\n",
       "together1285218221yu                     1\n",
       "ohwell                                   1\n",
       "waspfacts                                1\n",
       "fleekate                                 1\n",
       "speeding                                 1\n",
       "ipostdickprints                          1\n",
       "sistas                                   1\n",
       "_creamm_                                 1\n",
       "zoeboi03                                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rarest words\n",
    "\n",
    "rare = pd.Series(' '.join(data_tweet_str_wo_punc_wo_stop_wo_http_wo_no).split()).value_counts()[-50:]\n",
    "rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        mayasolovely woman complain cleaning house man...\n",
       "1        mleew17 boy dats coldtyga dwn bad cuffin dat h...\n",
       "2        urkindofbrand dawg ever fuck bitch start cry c...\n",
       "3                 c_g_anderson viva_based look like tranny\n",
       "4        shenikaroberts shit hear might true might fake...\n",
       "                               ...                        \n",
       "24778    yous muthafin lie corey_emanuel right tl trash...\n",
       "24779    youve gone broke wrong heart baby drove rednec...\n",
       "24780      young buck wanna eat dat nigguh like fuckin dis\n",
       "24781                    youu got wild bitches tellin lies\n",
       "24782    ruffled ntac eileen dahlia beautiful color com...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the rarest words\n",
    "\n",
    "data_tweet_str_wo_punc_wo_stop_wo_http_wo_no_wo_rare = data_tweet_str_wo_punc_wo_stop_wo_http_wo_no\\\n",
    "                                                        .apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    "data_tweet_str_wo_punc_wo_stop_wo_http_wo_no_wo_rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        mayasolovely woman complain cleaning house man...\n",
       "1        mleew17 boy dat coldtyga dwn bad cuffin dat ho...\n",
       "2        urkindofbrand dawg ever fuck bitch start cry c...\n",
       "3                 c_g_anderson viva_based look like tranny\n",
       "4        shenikaroberts shit hear might true might fake...\n",
       "                               ...                        \n",
       "24778    yous muthafin lie corey_emanuel right tl trash...\n",
       "24779    youve gone broke wrong heart baby drove rednec...\n",
       "24780      young buck wanna eat dat nigguh like fuckin dis\n",
       "24781                       youu got wild bitch tellin lie\n",
       "24782    ruffled ntac eileen dahlia beautiful color com...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "lemmatization = data_tweet_str_wo_punc_wo_stop_wo_http_wo_no_wo_rare\\\n",
    "                .apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-06689ca3b887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sentiment analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatization\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Sentiment'"
     ]
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "\n",
    "pd.DataFrame(lemmatization[:5]).Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [mayasolovely, woman, complain, cleaning, hous...\n",
       "1        [mleew17, boy, dat, coldtyga, dwn, bad, cuffin...\n",
       "2        [urkindofbrand, dawg, ever, fuck, bitch, start...\n",
       "3           [c_g_anderson, viva_based, look, like, tranny]\n",
       "4        [shenikaroberts, shit, hear, might, true, migh...\n",
       "                               ...                        \n",
       "24778    [yous, muthafin, lie, corey_emanuel, right, tl...\n",
       "24779    [youve, gone, broke, wrong, heart, baby, drove...\n",
       "24780    [young, buck, wan, na, eat, dat, nigguh, like,...\n",
       "24781                [youu, got, wild, bitch, tellin, lie]\n",
       "24782    [ruffled, ntac, eileen, dahlia, beautiful, col...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "\n",
    "tokens = lemmatization.apply(lambda x: word_tokenize(str(x)))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mayasolovely woman complain cleaning house man always take trash \n",
      "\n",
      "['mayasolovely', 'woman', 'complain', 'cleaning', 'house', 'man', 'always', 'take', 'trash']\n"
     ]
    }
   ],
   "source": [
    "# check results of tokenization\n",
    "\n",
    "print(lemmatization[0], '\\n')\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(mayasolovely, RB), (woman, NN), (complain, V...\n",
       "1        [(mleew17, NN), (boy, NN), (dat, NN), (coldtyg...\n",
       "2        [(urkindofbrand, NN), (dawg, NN), (ever, RB), ...\n",
       "3        [(c_g_anderson, NN), (viva_based, VBD), (look,...\n",
       "4        [(shenikaroberts, NNS), (shit, VBD), (hear, JJ...\n",
       "                               ...                        \n",
       "24778    [(yous, JJ), (muthafin, NN), (lie, NN), (corey...\n",
       "24779    [(youve, NN), (gone, VBN), (broke, VBD), (wron...\n",
       "24780    [(young, JJ), (buck, NN), (wan, WP), (na, TO),...\n",
       "24781    [(youu, NN), (got, VBD), (wild, JJ), (bitch, N...\n",
       "24782    [(ruffled, VBN), (ntac, RB), (eileen, JJ), (da...\n",
       "Name: tweet, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags to identify the parts of speech\n",
    "# part of the lemmatization process\n",
    "\n",
    "tags = tokens.apply(lambda x: nltk.pos_tag(x))\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
